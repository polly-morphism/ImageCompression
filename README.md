  1. PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.

  To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. 

  2. In wavelet transform, a function S(x) is represented as a superposition of a set of such wavelets or basis functions. These basis functions are obtained from a single prototype wavelet called the mother wavelet ψ(x) by dilations(scaling) and translations(shifts).

  For each n,k ∈ Z, we define ψn,k(x) as ψn,k(x) = 2n/2ψ(2nx−k) ψ(x) is a wavelet and the collection {ψn,k(x)} n,k∈Z is a wavelet orthonormal basis on R and ψ must satisfy ∫ ψ(x)dx =0.
